<meta charset="utf-8" emacsmode="-*- markdown -*-">
<link rel="stylesheet" href="site.css">

**Assignment 4**

_Advanced Computer Graphics (CMSC740), Fall 2025_

In this assignment, you'll explore inverse rendering with two different, physics-based rendering approaches: differentiable [path tracing](https://www.pbr-book.org/3ed-2018/Light_Transport_I_Surface_Reflection/Path_Tracing) and differentiable [neural radiosity](https://arxiv.org/abs/2105.12319). You will perform "toy experiments" where you will recover the diffuse BRDF parameters of some scene elements based on a given ground truth image of the scene.

!!! NOTE
    We provide a reference path tracer implementation to assist you with this assignment on request, which also include some skeleton code for neural radiosity. This code is linked on the assignment page on ELMS!

!!! NOTE
    Although we introduce scene parameter optimization in Part 1, 2 and network training in Part 3, the number of parameters being optimized is small. Therefore, it should work well and take a reasonable amount of time to run all the experiments on the CPU.

# 1. Inverse Rendering (30pts)

## Background

In general, we can view the process of rendering as a function $F$ taking in scene parameters $X$ (mesh locations, BRDF parameters, camera parameters, light sources, etc.), and producing pixel colors. The resulting image $I$ is simply written as

\begin{align}
I = F(X).
\end{align}

Inverse rendering solves the following problem: if we are given a reference image of the scene $I^{*}$, how to obtain unknown scene parameters $X$ that produce this reference image? In practice, $I^{*}$ is often a photograph (or many photos from different angles), and the ultimate goal is to reconstruct all scene parameters. In this assignment, you will work with rendering functions $F$ that solve the rendering equation, such as a path tracer or neural radiosity. However, to keep things simple your inverse rendering experiments will be "toy problems": you will only recover some diffuse BRDF parameters and assume everything else is known (geometry, light sources, camera parameters).

For inverse rendering, the difference between the desired ground truth image $I^{*}$ and the rendered image $F(X)$ needs to be quantified using a scalar loss function $L(X)$,

\begin{align}
L(X) &= d(F(X) - I^{*}),
\end{align}

where, $d$ is a [norm](https://en.wikipedia.org/wiki/Norm_%28mathematics%29) such as the $L_1$ or $L_2$ norm. $L$ is called the image reconstruction loss. Now, our goal is to optimize the parameters $X$ (or a subset thereof) to minimize the loss $L(X)$. This optimization process to minimize the loss function is called **training** in machine learning terminology.

Mathematically, it would be very convenient if $F$ is differentiable. Then we can compute the gradient and use that to update $X$ using iterative [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) to perform the optimization. That is,

\begin{align}
X_{i+1} \leftarrow X_{i} - \lambda \nabla L(X_{i}),
\end{align}

where $\lambda$ is a hyperparameter to control how much we update $X$ each time.  While this equation illustrates the high level idea of the optimization process, variations of gradient descent are typically used in practice. The most popular choice is often the so called [Adam](https://arxiv.org/pdf/1412.6980) algorithm, which we'll also use later.

Conveniently, machine learning libraries like PyTorch can automatically compute the gradients for you using [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation), which is also called [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) in the context of neural networks. If you have little experience with machine learning, it's recommended that you watch [this series of videos](https://www.youtube.com/watch?v=aircAruvnKk) to get a better understanding, and then follow [this tutorial](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html) to see how the ideas are translated to code.

!!! NOTE
    You won't be asked to write the training / optimization loop. Watching and reading the materials will help you better understand what is going on.

## Setup

In Aris, the rendering function $F$ is implemented by the integrator. So far, we've been working with completely known scene parameters, all defined in the corresponding `yaml` config files. For the experiments here, we prepared an all-diffuse scene `cbox_diffuse`. For this modified `cbox` scene, if you open `config/scene/brdf/cbox_diffuse.yaml`, you'll see the type and parameters of all the materials. You can map them to the corresponding meshes in `config/scene/cbox_base.yaml`:

```
# scene/cbox_base.yaml
- data/meshes/cbox/walls.obj
- data/meshes/cbox/rightwall.obj
- data/meshes/cbox/leftwall.obj
- data/meshes/cbox/sphere1.obj
- data/meshes/cbox/sphere2.obj
- data/meshes/cbox/light.obj

# scene/brdf/cbox_diffuse.yaml
- name: diffuse
  config:
    color: [0.725, 0.71, 0.68]
- name: diffuse
  config:
    color: [0.161, 0.133, 0.427]
- name: diffuse
  config:
    color: [0.630, 0.065, 0.05]
- name: diffuse
  config:
    color: [0.8, 0.8, 0.4]
- name: diffuse
  config:
    color: [0.4, 0.8, 0.8]
- name: diffuse
  config:
    color: [0.8, 0.8, 0.8]
```

So, you can see that the 2nd and 3rd diffuse BRDF have the colors of the left and right walls. The scene looks like this:

![Reference](images/a5/cbox-diffuse.png)

To get started with inverse rendering, we need to download the rendered **ground truth image** from [here](images/a5/cbox-diffuse.exr). This is the image we will try to match using inverse rendering. Note that it is an `exr` file. Put the file under the `scripts` folder (create it yourself, it is excluded from git).

```bash
# Command to generate reference, you don't need to run this
python render.py scene=cbox_diffuse integrator=path_mis spp=512 save_exr=true
```

!!! NOTE
    We use EXR file because it stores the original output (radiance) from the integrator, also known as a linear color space. The PNG image has the pixel values tone-mapped to [0, 1] in the sRGB color space for display.

## Reconstruct the Diffuse BRDFs of the Walls (10pts)

In this exercise you will now assume the diffuse BRDFs of the walls in the cbox scene are unknown, and you will reconstruct them by inverse rendering to match the ground truth image. First, read `train.py` and follow the comments to get an idea of how training works. Ask about whatever is unclear on Piazza. Also, check `aris/config/train_config.py` and `config/train_render.yaml` to understand what command line options are available.

!!! NOTE
    For this part, ignore the `mode == "nerad"` parts. They'll be explained in the last part of this assignment.

Now, take a look at `config/scene/brdf/cbox_train_diffuse.yaml`. The materials of the walls are replaced by `learned_diffuse`. The source code is in `aris/brdf/learned_diffuse.py`. It's almost identical to `diffuse.py`, with two important differences:

1. The `LearnedDiffuseBrdf` class is a subclass of `nn.Module`. This is the general practice in PyTorch to model trainable objects (see [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)), that is, the scene parameters that we will optimize. PyTorch will use automatic differentiation to compute gradients with respect to these trainable objects.
1. The color tensor is no longer given as input. Instead, they're initialized to gray and are trainable (`requires_grad=True`) `nn.Parameter` objects.

!!! NOTE
    In the rest of this assignment, feel free to modify train.py and other provided code to fit your needs. Make sure you document the important changes in your report.

You can now run the following command to try optimizing the color of the walls such that the rendered image matches with the ground truth. By default, training runs for 4,000 gradient descent steps, and a full image is rendered every 1,000 steps to show the progress. **Windows users: type the command in one line, or replace \ by `**

```bash
python train.py --config-name train_render \
    scene=cbox_train_diffuse \
    gt=./scripts/cbox-diffuse.exr
```

You'll see a progress bar showing not only the remaining time, but also the latest loss. To monitor the progress, you can also run Tensorboard in the output folder:

```bash
tensorboard --logdir tensorboard
```

After training (several minutes depending on your machine), you can find in the output folder:

- `checkpoints`, the optimized parameters (in this case, colors of the two walls)
- `tensorboard`, folder of tensorboard data (we log the losses over training)
- `visualization`, renderings of the image at different steps

The renders in the `visualization` folder use a lower number of samples per pixel (spp) than usual (default is 32) to reduce time consumption during training. To get a better quality rendering with your trained models, you need to load them in `render.py`.

!!! NOTE
    You should take a note of the path to the `ckpt` file you just trained. Better, copy it to a safe location, such as a folder tracked by git.

## Loading the Trained Models and Rendering Them (20pts)

The provided training code saves the optimized parameters to `ckpt` files periodically. To use them for rendering with other configurations (spp, integrator, etc.), implement checkpoint loading in `render.py`. You need to do the following:

1. Add a `checkpoint` option in `RenderConfig` that has type `Optional[str]`
1. Add a line `checkpoint:` to `config/render.yaml`, so by default no checkpoint is specified
1. In the `main()` function of `render.py`, :
    1. Get all the scene components that are trained, and call `.train(False)`
        1. Hint: refer to `train.py` but don't create optimizers
    1. If `cfg.checkpoint` is not None, load the ckpt file and call `load_state_dict` function of the trained components
    1. To prevent expensive gradient computation, add a `@torch.no_grad()` annotator to the `render_block()` function in `render.py`

Then, load your trained model and render an images with higher spp.

```bash
python render.py scene=cbox_train_diffuse \
    integrator=path_mis \
    spp=128 \
    checkpoint=./scripts/cbox-diffuse-walls-step4000.ckpt
```

![trained-128spp](images/a5/cbox-train-walls-step4000-spp128.png)

Now, render another image with the `albedo` integrator, which visualizes the color of the BRDF. For a better comparison, also render the albedo with the 2000-step checkpoint:

```
python render.py scene=cbox_train_diffuse \
    integrator=albedo \
    spp=32 \
    checkpoint=./scripts/cbox-diffuse-walls-step2000.ckpt

python render.py scene=cbox_train_diffuse \
    integrator=albedo \
    spp=32 \
    checkpoint=./scripts/cbox-diffuse-walls-step4000.ckpt
```

You can see how the optimization pulls the albedo (diffuse BRDF) towards the ground truth, but has not arrived yet:

![albedo-2000](images/a5/cbox-train-walls-step2000-albedo.png height="300px") ![albedo-4000](images/a5/cbox-train-walls-step4000-albedo.png height="300px") ![albedo-gt](images/a5/cbox-diffuse-albedo.png height="300px")

If you are interested, you can experiment with longer trainings and different learning rates.

# 2. Limiting Path Length (20pts)

`cbox` is just a simple example, and in real-world applications the scenes are generally much more complicated. Therefore, it would be desirable to limit the number of bounces that a ray can travel, which would limit computation time. As an extreme example, we could allow rays to bounce only once at the first hit point of the camera ray. In other words, the ray will hit at most two objects.

To implement this, repeat the above training experiments, but limiting `max_path_length` to `2` in the path tracer (see command below, we provide a `path_mis_clip.yaml` that sets it already). Training should run faster because the rays are terminated at second bounce.

```bash
python train.py --config-name train_render \
    integrator=path_mis_clip \
    scene=cbox_train_diffuse \
    gt=./scripts/cbox-diffuse.exr
```

Then, render the albedo map as before:

```bash
python render.py scene=cbox_train_diffuse \
    integrator=albedo \
    spp=32 \
    checkpoint=./scripts/clip-step4000.ckpt
```

Compare the results against ground truth and what you got from Part 1. What do you observe (hint: brightness)? Why does this happen? Discuss this in your report.

# 3. Neural Radiosity (50pts)

The [Neural Radiosity](https://arxiv.org/abs/2105.12319) technique (as discussed in the class) accounts for global illumination, but by creating only paths of length two similar as above. In this experiment, you will show that inverse rendering using neural radiosity indeed works to recover the diffuse BRDF.

## Training Neural Radiosity (15pts)

In the provided `path.py` code, we already implemented part of the `render_nerad()` function. In particular, the `LHS` computation is complete. You'll complete the `RHS` computation in the next section. But before that, you may have noticed some blanks that are left out in `train.py`. Fill in those blanks to enable training of neural radiosity.

After filling the blanks, launch training with:

```bash
python train.py --config-name train_nerad \
    scene=cbox_train_diffuse \
    gt=./scripts/cbox-diffuse.exr
```

Your code should run fine, and when you check the `visualization` folder you'll see `lhs.png` and `rhs.png` in each folder. Take a look at the images to make sure that everything is working fine. You don't need to include them for this part, as the implementation is incomlete.

You will notice that in the provided code, `RHS` is set to be the result of ordinary path tracing. This is just a placeholder to make sure that you can run training and get some reasonable results, but it's not how neural radiosity works. In fact, the results will be closer to Part 2. You will implement the actual RHS computation for neural radiosity in the next part.

## Running Neural Radiosity (25pts)

Remove the placeholder `RHS` computation and implement your own, using the actual neural radiosity approach. Launch training with the same command as before. Your training should be faster and more effective. Include the `lhs.png` and `rhs.png` files in your report.

## Rendering Neural Radiosity (10pts)

Finally, add the relevant code in `render.py` to load and render with a pretrained neural radiosity model.

!!! NOTE
    While in `train.py` we save both `lhs` and `rhs` renderings during visualization, for render purpose we only need to use `rhs` as output.

Making the following changes should be sufficient:

1. Add the `mode` option to `RenderConfig`
1. Init and load integrator from checkpoint in `nerad` mode
1. Modify `render_block()` to handle `nerad` mode

You should be able to run the following commands and produce (1) a high quality rendering of the trained scene, (2) albedo renderings.

```bash
python render.py scene=cbox_train_diffuse \
    mode=nerad \
    integrator=path_mis_clip \
    spp=128 \
    checkpoint=./scripts/nerad-step4000.ckpt

python render.py scene=cbox_train_diffuse \
    integrator=albedo \
    spp=32 \
    checkpoint=./scripts/nerad-step2000.ckpt

python render.py scene=cbox_train_diffuse \
    integrator=albedo \
    spp=32 \
    checkpoint=./scripts/nerad-step4000.ckpt
```

Example outputs (your results may vary):

![nerad-128spp](images/a5/nerad-step4000-spp128.png)

![albedo-2000](images/a5/nerad-step2000-albedo.png height="300px") ![albedo-4000](images/a5/nerad-step4000-albedo.png height="300px") ![albedo-gt](images/a5/cbox-diffuse-albedo.png height="300px")

You can try with different learning rates and number of steps to see how the results change.

<style class="fallback">body{visibility:hidden}</style><script>markdeepOptions={tocStyle:'medium'};</script>
<!-- Markdeep: --><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>

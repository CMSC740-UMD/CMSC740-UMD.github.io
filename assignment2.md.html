<meta charset="utf-8" emacsmode="-*- markdown -*-">
<link rel="stylesheet" href="site.css">

**Assignment 2**

_Advanced Computer Graphics (CMSC740), Fall 2023_

!!! WARNING
    This page is a draft. It's content is subject to change without notification until it's officially released. Feel free to have a preview but don't start working on it yet.

!!! NOTE
    **Before you start**, fetch the latest changes from the [Git repository](https://github.com/CMSC740-Fall22/aris-renderer-student), and merge them into your working branch. If you use VSCode, you can do this with a GUI after installing an extension. Below is an example with the [Git Graph](https://marketplace.visualstudio.com/items?itemName=mhutchie.git-graph) extension. Remember to fetch and merge the _latest_ commit from `origin/master`.

![Merge origin/master into your working branch](images/a2/git-example.png width="200px")

In the second assignment you'll:

- Write functions that sample from different distributions
- Implement two simple rendering algorithms

# 1. Distribution Transformation

In rendering, we often need random samples from various non-uniform probability distributions over different domains, such as squares, circles, spheres, or hemispheres etc. However, built-in functions in most systems and libraries only generate samples from uniform or normal distributions, and over (n-dimensional) cubes. Therefore, we have to transform samples from the given distributions (typically a uniform distribution over a 2D square, in our case) to the desired ones (non-uniform distribution over a desired domain such as a circle or sphere).

For example, in Assignment 1, you used `torch.rand` to generate uniformly distributed points on the square from $[0, 0]$ to $[1, 1]$. You then transformed them to a uniform distribution on a larger square from $[-1, -1]$ to $[1, 1]$. For this assignment, you will complete functions that tranform uniformly distributed points on the unit square square (from 'torch.rand') to a variety of other distributions and domains.

**Getting Started**

Open `aris/utils/sampling_utils.py`. You'll see a few `sample_*` functions and `pdf_*` functions.

- A `sample_distribution` function has one argument `uv`, which is an `(N, 2)` tensor containing $N$ 2D uniformly random samples in the $[0, 1]$ square. You can also view each 2D sample as two 1D samples ($u,v$). The function should return a `(N, D)` tensor of samples in the desired distribution and over the desired domain. $D$ is either 2 or 3.

- A `pdf_distribution` function takes in an `(N, D)` tensor of $N$ samples, and returns a `(N,)` tensor with the PDF (probability density function) values of each sample in that distribution. You do not need to check if the samples are actually inside the distribution (just assume that they are).

See the `uniform_square` example provided to you. It transforms the $[0, 1]$ square to the $[-1, 1]$ square, and every point in the square has PDF $p=0.25$ (this is because the integral of the PDF over the domain needs to be $1$, and the area of the $[-1, 1]$ square is $4$).

**Visualization**

Open `test_warp.ipynb` in VSCode. Run the first and second cells. You'll see an UI like this:

![Warping UI](images/a2/warping_ui.png)

This is a visualization of the points transformed by the `sample_uniform_square` function. The 2D samples are put on the $z=0$ plane. You can rotate the view with your mouse.

Later, when you have implmented other functions, you can click on the "uniform_square" dropdown and select a different distribution to verify your code (see examples below).

**Verification**

Run the third cell. You'll see a passing test followed by an error:

![Warping Test](images/a2/warping_test.png  height="120px")

This cell tests all sampling and PDF functions by checking if the density of the sampled points from the `sample_` functions match the returned values from the corresponding `pdf_` functions. It throws an error right now because those functions have not been implemented.

!!! NOTE
    While reading the sections below, also take a look at the report template for Assignment 2 which has more specific requirements.

## 1.1 Tent

Complete the `tent` functions for 2D samples in this distribution (the domain here is again the $[-1, 1]$ square, but the PDF is non-uniform):

\begin{equation*}
p(x,y)=p_1(x)p_1(y) \text{ where } p_1(t)=\begin{cases}1-|t|, &-1\le t\le 1 \\ 0, &\text{otherwise}\end{cases}
\end{equation*}

**Hints**

For sampling $p_1(t)$ follow these steps:

1. Compute the CDF (cumulative distribution function) $P_1(t)$
1. Derive the inverse $P^{-1}_1(t)$
1. Map the uniform sample with the inverse

You'll be asked to show your steps in the report (see report template for details).

## 1.2 Uniform Disk

Complete the `uniform_disk` functions for 2D samples uniformly distributed on a unit disk. The disk is centered at the origin and has radius 1.

## 1.3 Uniform Sphere

Complete the `uniform_sphere` functions for 3D samples uniformly distributed on a unit sphere. The sphere is centered at the origin and has radius 1.

For this and the following exercises, we use the spherical coordinates $(\theta, \phi)$ for describing PDFs and follow this convention of representation in the cartesian coordinates:

\begin{align*}
x &= \sin \theta \cos \phi \\
y &= \sin \theta \sin \phi \\
z &= \cos \theta
\end{align*}

Note that the output of your `sample_` functions and the inputs to your `pdf_` functions are 3D samples in cartesian coordinates.

**Hints**

The distribution in spherical coordinate representation is separable. Therefore, sampling of $\theta$ and $\phi$ from the two samples $u,v$ can be split into two steps:

1. Map $u$ to $\theta$
1. Map $v$ to $\phi$

## 1.4 Uniform Hemishpere

Complete the `uniform_hemisphere` functions for 3D samples uniformly distributed on the upper half of a unit sphere. The sphere is centered at the origin and has radius 1. Here, "up" means the direction of $(0, 0, 1)$, so upper half means $z\ge 0$.

## 1.5 Cosine Hemisphere

Complete the `cosine_hemisphere` functions for 3D samples on the upper half of a unit sphere with this density function:

\begin{equation*}
p(\theta, \phi)=\frac{\cos(\theta)}{\pi}
\end{equation*}

## 1.6 Beckmann

Complete the `beckmann` functions for 3D samples on the upper half of a unit sphere with this density function (we will later use this distribution to model glossy surfaces):

\begin{equation*}
p(\theta, \phi)=\frac{1}{2\pi}\cdot\frac{2e^{-\tan^2(\theta)/\alpha^2}}{\alpha^2\cos^3(\theta)}
\end{equation*}

Here, $\alpha$ is a parameter that controls the "smoothness" of the distribution, as shown below:

![Different alphas, from Nori](images/a2/beckmann.png  height="200px")

**Hints**

You may find these substitutions and identities useful:

\begin{align*}
x &= \cos\theta \\
\tan^{2}\theta &= \frac{1-x^{2}}{x^{2}} \\
\int f'(x)e^{f(x)}\text{d}x &= e^{f(x)}+C
\end{align*}

## Example output

You'll include screenshots of the plots for your implementation in the report. They should look like these (click to view larger):

![Tent](images/a2/vis-tent.png height="240px") ![Uniform Disk](images/a2/vis-uniform-disk.png height="240px") ![Uniform Sphere](images/a2/vis-uniform-sphere.png height="240px") ![Uniform Hemisphere](images/a2/vis-uniform-hemisphere.png height="240px")

![Cosine Hemisphere](images/a2/vis-cosine-hemisphere.png height="240px") ![Beckmann $\alpha=0.1$](images/a2/vis-beckmann-0.1.png height="240px") ![Beckmann $\alpha=0.5$](images/a2/vis-beckmann-0.5.png height="240px") ![Beckmann $\alpha=0.9$](images/a2/vis-beckmann-0.9.png height="240px")

And your code should pass the checks as well.

# 2. Point Light Integrator

![](images/a2/ajax-point.png height="400px")

Complete the point light integrator in `aris/integrator/point_light.py`. $\newcommand{xx}{{\bf x}}$

The point light integrator represents a simple rendering algorithm: assume that there is a single light source that emits energy towards all directions. Some of this light hits an object and gets reflected towards the camera.

Of course, in our ray tracing implementation we render backwards: starting from the camera, we shoot rays and see if they hit any object. If the ray hits a point $\xx$, we check if the light source is visible to $\xx$, i.e., if light can arrive at $\xx$ without obstruction to get reflected towards the camera. If so, this ray will carry some energy and be visible.

How do we compute this energy? For a point light located at ${\bf p}$ and emitting energy ${\bf\Phi}$, the formula is:

\begin{align*}
L(\xx) = \frac{{\bf\Phi}}{4\pi^{2}}\frac{\text{max}(0,\cos\theta)}{||\xx-{\bf p}||^{2}}V(\xx, {\bf p})
\end{align*}

$\theta$ is the angle between the direction from $\xx$ to $\bf p$ and the shading surface normal (`sh_normal` in code). $V$ is the visibility function, whose value is $1$ if the two points are mutually visible, and $0$ otherwise.

**Hints**

1. To find if the light source is visible to $\xx$, shoot a ray from $\xx$ to $\bf p$. In our setup, the light source is outside the bounding box of the object, so visibility can be determined by whether the ray hits anything. When you shoot the ray, you should also move the ray origin along the normal away from the surface a little bit. This avoids that ray tracing will return an intersection with $\xx$ itself (which could happen due to limited numerical precision).
1. There is a `dot` function in `aris.utils.tensor_utils` to help you save some typing.

!!! NOTE
    You can see how the point light properties are defined in the integrator. For this reason we need to have a separate integrator configuration for every scene. As in `config/integrator/point_ajax.yaml`:
    ```yaml
    name: point
    config:
      position: [-20, 40, 20]
      energy: [37600, 37600, 37600]
    ```

Render the Ajax scene with the following command:

```
python render.py scene=ajax_simple integrator=point_ajax
```

# 3. Ambient Occlusion Integrator

![](images/a2/ajax-ao.png height="400px")

Complete the ambient occlusion integrator in `aris/integrator/ao.py`.

In ambient occlusion rendering, imagine a diffuse object is placed inside a sphere with uniformly bright light coming from every direction. The brightness of a surface point is solely determined by how much of the sphere is visible to the point. Formally, the formula is:

\begin{align*}
L(\xx) = \int_{H^{2}(\xx)} V(\xx, \xx + \alpha\omega_i)\frac{\cos\theta}{\pi}\text{d}\omega_i
\end{align*}

Here, $H$ is the upper hemisphere centered at $\xx$; "up" here means the direction of the surface normal at $\xx$. $\theta$ is the angle between $\omega_i$ and the normal. $\alpha$ controls how far the occlusion effect is, and for this assignment you can assume $\alpha=\infty$.

Pay attention to $\frac{\cos\theta}{\pi}$, meaning that the samples have cosine weights on the hemisphere, which you have implemented in Part 1. However, your samples are in a coordinate system where the "up" direction is $(0,0,1)$. Therefore, you need to transform them to the coordinate system at $\xx$ defined by its normal. You can use `LocalFrames` in `aris.utils.coords_utils` to do the conversion.

!!! NOTE
    Reminder: every ray only evaluates one sample on the hemisphere. The integral is approximated by shooting many rays.

!!! NOTE
    Reduce spp and / or resolution to accelerate debugging

Render the Ajax scene with the following command:

```
python render.py scene=ajax_simple integrator=ao spp=512
```

<style class="fallback">body{visibility:hidden}</style><script>markdeepOptions={tocStyle:'medium'};</script>
<!-- Markdeep: --><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>
